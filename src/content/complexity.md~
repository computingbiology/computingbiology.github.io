+++
date = "26 Dec 2021"
draft = true
title = "Cost of Computation"
author = "David Evans"
slug = "complexity"
+++

When we consider the _power_ of our [computing models](/models), we
have been concerned with understanding what functions can be computed
(in theory, which means without any arbitrary limits on their
execution) by different computing models. When we want to actual
compute using physical computers, _cost_ matters. The main goal of
_complexity analysis_ is to understand the cost to perform different
computations.

For concerete computations where we already know the input and have an
implementation of the algorithm, we can measure the cost of executing
the algorithm by just running it and recording the cost. That cost
could be measured in _dollars_ (which is very natural and concrete,
now that you can use cloud computing to rent computers by the
[minute](https://aws.amazon.com/ec2/pricing/#:~:text=EC2%20usage%20are%20billed%20on,with%20a%2060%20second%20minimum.)
or [tenth of
second](https://azure.microsoft.com/en-us/pricing/details/functions/)),
and this makes sense if we need to perform a similar computation many
times in the same way. We could also measure computation cost in
_time_ (latency, how long it takes to go from input to output),
_throughput_ (how many times we can perform a computation in a time
period), and _memory use_ (how much memory is needed to complete the
computation). All of these could be converted to money cost, but the
actual conversion would depend on the available computing
infrastructure.





